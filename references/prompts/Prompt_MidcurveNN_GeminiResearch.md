## Prompt

```
You are an expert in Geometric Modeling algorithms as well as Artificial Intelligence. You are working on a problem called 'Midcurve Computation' and exploring which approaches are possible.

'Midcurve Computation' is essentially a dimension reduction problem on shapes. Graph Summarization/Dimension-Reduction/Compression: Reducing a large graph to a smaller graph preserving its underlying structure, similar to text summarization, which attempts to keep the essence.

Here,

- Input is 2D profile, say a closed polygon. Example: thick ‘L’ profile.
- Output is 1D curve in the middle of the profile.
- 2D shapes are represented as graph, where vertices as nodes, connecting curves as edges.

Wish to build encoder-decoder network which accepts graphs as input as well as output.
Training set of such input and output graphs pairs is available for any supervised approach.

Main considerations:

- The limitation with Seq2Seq models is notable; the input polygons and output branched midcurves are not linearly connected. They may exhibit loops or branches, necessitating further consideration for a more suitable solution.
- It poses a variable input and variable output challenge, given that the number of points and lines differs in the input and output.
- For Encoder-Decoder networks like those in Tensorflow, fixed-length inputs are necessary. Introducing variable lengths poses a significant hurdle as padding with a distinct unused value, such as 0,0, is not feasible, considering it could represent a valid point.

Similar to Seq2Seq models, are there graph2graph models available suitable for above problem? Any ideas?
```

## Response

### **Midcurve Computation via Graph-Based Deep Learning: An Exploration of Potential Approaches**

**1\. Introduction: Defining Midcurve Computation in the Realm of Graph-Based Dimension Reduction**

The problem of midcurve computation seeks to derive a one-dimensional curve that lies centrally within a given two-dimensional closed polygon. This task can be reformulated within the domain of graph theory and artificial intelligence as a dimension reduction problem on shapes. Specifically, if a two-dimensional profile, such as a thick ‘L’ shape, is represented as a graph where vertices correspond to points and edges represent the connecting lines or curves, then the goal of midcurve computation becomes the reduction of this larger graph to a smaller, one-dimensional graph that preserves the essential structural characteristics of the original shape 1. This process bears a strong resemblance to graph summarization or compression, where a complex graph is simplified into a more manageable representation while retaining its fundamental structure, much like how text summarization extracts the core meaning from a lengthy document 1.

This exploration acknowledges the inherent complexities associated with this problem, particularly those highlighted. The input polygons and the resulting branched midcurves are not always linearly connected; they can exhibit loops or branches, necessitating sophisticated modeling approaches beyond traditional sequential methods \[User Query\]. Furthermore, the challenge of variable input and output arises from the fact that the number of points and lines defining the two-dimensional input profile will likely differ from those defining the one-dimensional output midcurve 2. Traditional sequence-to-sequence models, which often rely on a linear progression of data, are not inherently suited to handle these non-linear graph structures 3. Moreover, standard encoder-decoder networks, commonly implemented in frameworks like TensorFlow, typically require fixed-length input vectors. Directly applying such architectures to graphs with a variable number of nodes and edges presents a significant obstacle. While padding is a common technique to handle variable-length sequences, padding with a specific coordinate value like (0,0) is not a viable solution in this context, as such a coordinate could represent a legitimate point within the geometric profile \[User Query\].

The fundamental difficulty lies in identifying a graph-to-graph transformation that can effectively manage the change in dimensionality from a 2D boundary to a 1D skeletal representation. This transformation must also accommodate potential alterations in topology, such as the emergence of loops or branches in the midcurve, and handle the variations in the size of the input and output graphs, all while respecting the underlying geometric constraints of the shape \[Insight 1.1\].

**2\. Fundamentals of Graph Neural Networks for Generative Tasks**

Graph Neural Networks (GNNs) have emerged as a powerful class of neural networks specifically designed to operate on data structured as graphs 6. Unlike traditional neural networks that expect data in regular grid formats (like images) or sequences (like text), GNNs can directly process graphs, making them well-suited for tasks involving interconnected entities 6. A graph (G) is formally defined by a set of nodes (vertices) (V) and a set of edges (E) that connect these nodes 6. In the context of geometric modeling, the nodes of a graph representing a 2D profile could correspond to the vertices of the polygon, and the edges could represent the line segments connecting them. Furthermore, nodes can possess features, represented by a node feature matrix (X), which in this case could include the x and y coordinates of each vertex 6. The development of GNNs was motivated by the limitations of applying conventional CNNs to graph data, which often exhibits arbitrary sizes and complex, irregular connectivity patterns 6.

Several categories of GNNs are particularly relevant to the task of graph generation 7. **Graph Convolutional Networks (GCNs)** learn representations of nodes by aggregating feature information from their local neighborhoods 6. This aggregation process typically involves a convolution operation adapted for graph structures, followed by linear transformations and non-linear activation functions 6. GCNs effectively generalize the concept of convolution to non-Euclidean graph data, making them suitable for processing geometric shapes represented in this manner 8. **Graph Autoencoders (GAEs)**, on the other hand, employ an encoder to learn a low-dimensional embedding of the input graph and a decoder to reconstruct the original graph from this embedding 7. Variational Graph Autoencoders (GraphVAEs) extend this by learning a probabilistic distribution over the latent embeddings, enabling the generation of new graphs by sampling from this distribution 10. **Recurrent Graph Neural Networks (RGNNs)** and **Gated Graph Neural Networks (GGNNs)** are designed to handle graphs with sequential or temporal dependencies and can manage multi-relational graphs where nodes can be connected by different types of edges 7. GGNNs, in particular, utilize gating mechanisms to better capture long-range dependencies, which could be beneficial for generating complex, branched midcurves 7.

A fundamental capability of GNNs is their ability to learn **node embeddings**. These are low-dimensional vector representations of each node in the graph, where nodes that are similar in terms of their structural role or features are embedded close to each other in the embedding space 6. These embeddings can encapsulate information about the node itself, its connections (edges), and the broader context within the graph 7. The core operation in many GNNs is **message passing**, where nodes exchange information with their neighbors. This involves each node aggregating the features of its neighbors and then updating its own feature representation based on this aggregated information and its previous state 12. This iterative process, repeated over multiple layers of the GNN, allows the network to capture increasingly complex relationships and dependencies within the graph 11. The functions used for aggregating information from neighbors are typically designed to be order-invariant (e.g., sum, average, maximum) to account for the arbitrary ordering of nodes in a graph 6.

The capacity of GNNs to directly process and learn from graph-structured data makes them a promising framework for the midcurve computation problem \[Insight 2.1\]. The variety of GNN architectures offers a range of tools that could be adapted for this generative task. GCNs can be employed to extract relevant features from the 2D profile graph, while GAEs and GraphVAEs provide a foundation for learning compressed representations and generating new graphs. RGNNs and GGNNs might be useful for handling the sequential aspects of graph generation or the complex branching patterns in midcurves. The message-passing mechanism inherent in GNNs allows for the integration of both local and global information, which is essential for understanding the geometric and topological characteristics of the input shape.

**3\. Exploring Graph Encoder-Decoder Architectures for Graph Transformation**

Graph encoder-decoder architectures offer a potential solution for transforming one graph into another, making them relevant for the midcurve computation problem \[Insight 3.1\]. These models generally comprise two key components: an **encoder** and a **decoder** 14. The encoder takes the input graph, which in this case is the 2D profile represented as a graph, and maps it into a fixed-length, continuous vector known as the latent embedding 14. The primary objective of the encoder is to capture the essential structural and geometric information of the input graph within this lower-dimensional representation. Designing an encoder that can effectively handle input graphs of varying sizes and produce a consistent, fixed-size embedding is a significant challenge 14.

The **decoder** then takes this latent embedding as input and generates the output graph, which is the 1D midcurve 14. The decoder must be capable of creating a graph that may have a different number of nodes and edges compared to the input graph and should exhibit the desired one-dimensional structure, potentially including loops or branches. The process of decoding a graph from a latent embedding is generally more intricate than decoding a sequence 14.

In the context of midcurve computation, the encoder can utilize GNNs, such as GCNs or GraphSAGE, to process the 2D profile graph 7. These GNNs can iteratively aggregate information from the nodes and their neighbors, learning node embeddings that capture local and global structural properties. These node embeddings can then be combined, for instance, through pooling operations, to produce a single, fixed-size graph-level embedding that represents the entire 2D profile 12. Graph autoencoders inherently possess an encoder component that learns such a latent representation by attempting to reconstruct the input graph, although for midcurve computation, the reconstruction target would be the midcurve itself, not the original profile 7.

The decoder's task is to take this latent embedding and generate the 1D midcurve graph. This involves determining the number of nodes and edges in the output graph, their features (such as coordinates), and their connectivity (adjacency) 14. One potential approach is to employ a sequential generation process, where the decoder iteratively adds nodes and edges to construct the midcurve graph, conditioned on the latent embedding 16. This could involve using recurrent GNNs or similar autoregressive mechanisms. Another strategy is to directly predict the adjacency matrix and the node features of the output graph from the latent embedding. However, managing variable graph sizes with this direct prediction method can be complex. The decoder might also be designed to predict a set of parameters that define the 1D curve, which are subsequently used to construct the graph representation of the midcurve.

The primary hurdle in employing graph encoder-decoder architectures for midcurve computation lies in the design of both the encoder and the decoder. The encoder must effectively capture the geometric essence of the 2D shape into a fixed-length vector, while the decoder must be capable of generating a variable-sized 1D graph with the appropriate topology and geometry from this latent representation.

**4\. Addressing the Challenge of Variable-Sized Graph Input and Output**

A significant challenge in applying deep learning to graph data, particularly in generative tasks like midcurve computation, is the inherent variability in the number of nodes and edges across different input and output graphs 2. Standard neural network architectures often require fixed-size inputs, which necessitates specific strategies when dealing with graphs. Several approaches within the realm of graph neural networks aim to address this challenge 2.

**Graph Pooling** is one such technique, where the size of the graph representation is progressively reduced as it passes through the network 12. Pooling layers in GNNs can aggregate information from multiple nodes into a smaller set, eventually leading to a fixed-size embedding in the encoder, irrespective of the initial input graph size. Various pooling strategies exist, including node selection based on importance scores or feature aggregation based on clustering or other criteria.

**Sequential Graph Generation** offers another way to handle variable output sizes 7. In this approach, the decoder generates the output graph step-by-step, one node or edge at a time. This allows for the creation of graphs with a number of nodes and edges that is not predetermined. Recurrent GNNs (RGNNs) or other autoregressive models can be employed for this, where the generation of each new element is conditioned on the previously generated elements and the latent embedding from the encoder.

The concept of **Learning Graphons** provides a more theoretical approach 14. Graphons are mathematical objects that can represent the limit of a sequence of dense graphs. While practically complex to implement, the idea is to encode a finite graph into a graphon and then sample a graph of a desired size from it. This could potentially address the variable size issue by decoupling the size of the generated graph from the size of the latent representation.

**Hybrid Approaches** that combine GNNs with sequential models like RNNs or Transformers could also be beneficial 2. The GNN can be used to process the input 2D profile graph and extract a fixed-size feature vector, which is then fed into a sequential model. The sequential model can then generate the 1D midcurve graph as a sequence of nodes and edges or as a sequence of parameters defining the curve.

For the encoder to produce a fixed-length embedding from a variable-sized input graph, global pooling operations are commonly used 12. After processing the input graph through several GNN layers, the resulting node embeddings can be aggregated using functions like sum, mean, or max pooling to obtain a single vector that represents the entire graph. Alternatively, some graph embedding techniques are specifically designed to produce fixed-size embeddings for graphs of different sizes, such as methods based on random walks or subgraph sampling.

The decoder, to generate a variable-sized output graph representing the midcurve, can employ several strategies. One is to predict the number of nodes and edges in the target midcurve as an initial step, possibly conditioned on the latent embedding, and then proceed to generate the graph accordingly 15. Another approach involves an autoregressive generation process where the decoder iteratively decides whether to add a new node, add an edge between existing nodes, or terminate the generation process 16. These decisions are based on the current state of the partially generated graph and the latent embedding. The decoder could also generate a sequence of actions that construct the graph, such as specifying the coordinates of a new node and its connections to the existing graph.

A particularly relevant idea from the research is to encode the number of nodes or edges of the target graph directly into the latent embedding produced by the encoder 15. For instance, a Reddit user suggested that the encoder could output a fixed-size vector that also includes information about the number of nodes in the expected midcurve. The decoder could then use this information to guide the generation process, for example, by predicting the existence of edges between pairs of nodes based on the embedding and the encoded number of nodes.

Addressing the variable size challenge effectively likely requires a combination of these techniques \[Insight 4.1\]. The encoder needs to be robust to variations in input size and condense the essential geometric structure into a fixed-length representation. The decoder needs the flexibility to generate graphs of varying sizes and complexities, possibly through sequential generation guided by information about the target graph's size encoded in the latent space.

**5\. Graph Summarization with Neural Networks: A Pathway to Midcurve Computation**

The task of midcurve computation shares a fundamental objective with graph summarization: the reduction of complexity while preserving essential information 1. In our case, we aim to reduce a two-dimensional profile, represented as a graph, to its one-dimensional skeletal structure, also represented as a graph. This reduction must retain the core shape and its medial characteristics.

Neural network-based graph summarization techniques offer a promising avenue for tackling this geometric dimension reduction task \[Insight 5.1\]. Graph Neural Networks (GNNs) have become increasingly utilized for graph summarization due to their capacity to learn meaningful representations from large and intricate graph datasets 19. The primary goal of graph summarization is to produce a condensed representation (summary) of a graph that still captures its key properties 19. In the context of midcurve computation, the "key property" to be preserved is the overall geometric shape and its underlying medial structure.

Traditional graph summarization methods often involve grouping nodes or edges based on certain criteria, such as the density of connections, or selecting a representative subset of the original graph 19. Neural approaches, particularly those employing GNNs, can learn these summarization rules in an end-to-end manner, allowing for more sophisticated and context-aware summarization.

Graph autoencoders provide a particularly relevant framework for graph summarization 7. By learning a low-dimensional latent representation of the input graph and then attempting to reconstruct it, the autoencoder is forced to capture the most salient information in the bottleneck layer. This compressed representation can be considered a summary of the original graph. For midcurve computation, the autoencoder could be trained such that the reconstructed graph represents the 1D midcurve corresponding to the input 2D profile.

The concept of "supernodes" in graph summarization, where a group of original nodes is aggregated into a single, representative node, might also be applicable 18. If the 2D profile can be segmented into meaningful parts that correspond to segments of the midcurve, then each of these parts could potentially be represented by a supernode in a summarized graph (the midcurve). The connections between these supernodes would then represent the connectivity of the midcurve segments.

It is worth noting that various fields use different terms, such as "coarsening," "reduction," "simplification," "abstraction," and "compression," to refer to processes similar to graph summarization 19. All these concepts align with the goal of midcurve computation, which is essentially to obtain a simplified, lower-dimensional representation of the original geometric shape. Therefore, exploring and adapting existing neural graph summarization models, especially those based on autoencoders or learned pooling mechanisms, could be a direct and effective approach to the midcurve computation problem. The key would be to ensure that the summarization process prioritizes the preservation of the geometric essence of the shape.

**6\. Geometric Deep Learning on Graphs: Incorporating Geometric Information**

While standard Graph Neural Networks (GNNs) excel at processing the topological structure of graphs, midcurve computation inherently relies on the geometric properties of the input shape 1. The spatial arrangement of points and lines defining the 2D profile is crucial for determining the location and shape of the 1D midcurve. Geometric deep learning extends deep learning techniques to non-Euclidean data like graphs and meshes, explicitly considering the underlying geometric relationships and symmetries 22. This field provides the necessary tools to incorporate geometric information into our graph-based approach.

In our representation, the nodes of the graph can represent points in 2D space, and their features can directly include their x and y coordinates 6. The edges connecting these nodes, representing the lines or curves of the shape, can have features such as their length (the Euclidean distance between the connected nodes) 6. For more complex shapes or midcurves involving curves, edge features could also encode information about curvature or other geometric properties.

Several GNN architectures are specifically designed to process geometric information on graphs 22. **Geometric Convolutional Neural Networks (GCNNs)** extend the concept of convolution to irregular, graph-structured data, allowing them to learn from the spatial relationships between points 22. Models incorporating **E(3) equivariance**, while primarily used for 3D shapes, demonstrate the importance of handling spatial transformations and could inspire similar approaches for 2D geometric problems 24. **Graph Attention Networks (GATs)** can be particularly useful as their attention mechanisms allow the network to weigh the importance of different neighboring nodes based on their relevance to the task 11. In midcurve computation, this could enable the model to focus on geometrically significant parts of the 2D profile.

The application of geometric deep learning to molecular modeling, where molecules are represented as graphs with atoms as nodes (having spatial coordinates) and bonds as edges, provides a relevant example 22. Similarly, research on mesh segmentation using hybrid graphs and node-edge-face embeddings shows how GNNs can effectively capture complex geometric interactions by considering different geometric primitives within the graph structure 24.

For the midcurve computation problem, it is therefore essential to move beyond simply representing the 2D profile as a topological graph. By incorporating the spatial coordinates of the vertices as node features and potentially the lengths or other properties of the edges as edge features, and by utilizing GNN architectures that are designed to be sensitive to these geometric attributes, the model can learn a more accurate and geometrically meaningful mapping between the 2D profile and its 1D midcurve \[Insight 6.1\]. This integration of geometric information is crucial for ensuring that the generated midcurve accurately reflects the "middle" of the input shape in a spatial sense.

**7\. Handling Non-Linear Structures: Loops and Branches in the Generated Midcurve**

A critical requirement for a successful midcurve computation model is the ability to generate one-dimensional curves that can exhibit non-linear structures such as loops and branches \[User Query\]. The medial axis or centerline of a complex two-dimensional shape often contains these topological features, and the model must be capable of reproducing them in the output graph.

Several graph generation models offer the potential to produce such complex graph structures 16. **Sequential Graph Generation with Decision Points** involves building the graph step by step, where at each step, the model decides whether to extend the current path, create a new branch, or close a loop 16. This decision-making process can be guided by the latent embedding of the input shape and the current state of the generated midcurve. Recurrent neural networks or similar sequential models in the decoder could be employed for this purpose.

Drawing inspiration from workflow management, the graph generation process could be viewed as a sequence of steps with the ability to branch into parallel paths or loop back to previous steps based on certain conditions 26. This might involve defining specific types of events (e.g., start branch, end loop) and transitions within the graph generation process.

**Autoregressive Graph Generation** provides another viable approach 16. Similar to how language models generate text word by word, a graph can be generated by sequentially predicting the addition of nodes and edges based on the previously generated parts of the graph and the latent embedding. This method can potentially handle the creation of complex topologies, including branches and cycles.

Generative Adversarial Networks (GANs) have also been applied to graph generation 10. These models consist of a generator network that learns to produce graphs that resemble a target distribution and a discriminator network that tries to distinguish between generated and real graphs. By training these two networks in an adversarial manner, the generator can learn to produce graphs with complex structures if such structures are present in the training data.

Specifically, a decoder based on a **Recurrent Graph Neural Network (RGNN)** could be particularly well-suited for generating branched structures 7. The RGNN can maintain an internal state as it generates the midcurve graph. At each step, based on the current state and the latent embedding of the input shape, the RGNN can decide to add a new node and connect it to existing nodes, potentially creating a branch. The recurrent nature of the model allows it to remember previous decisions and generate coherent, non-linear structures. The generation process could iteratively involve deciding whether to add a new node or a new edge, conditioned on the current state of the graph and the encoded representation of the input 16.

Generating midcurves with loops and branches necessitates models that can make intricate decisions during the generation process, going beyond a simple linear progression \[Insight 7.1\]. Sequential graph generation models, especially those incorporating recurrent mechanisms or explicit decision-making steps for branching and looping, appear to be the most promising avenues for achieving this capability.

**8\. The Role of Attention Mechanisms in Graph Generation for Midcurves**

Attention mechanisms within Graph Neural Networks (GNNs) can play a crucial role in the midcurve computation problem by enabling the model to focus on the most relevant parts of the input 2D profile graph when generating the output 1D midcurve 11. These mechanisms allow the network to learn the importance of different nodes and edges in the input graph and to weigh their contributions accordingly when making decisions about the structure and geometry of the output midcurve.

For complex 2D shapes, the path of the midcurve in one region might be influenced by the geometry of the boundary in a seemingly distant region. Attention mechanisms can help the model capture these long-range dependencies by allowing nodes in the GNN to "attend" to other nodes that are topologically or geometrically far away but are nevertheless important for determining the midcurve 25.

In Graph Attention Networks (GATs), the attention mechanism is specifically used to weigh the contributions of different neighboring nodes when aggregating their features 11. This allows the model to learn which neighbors are most important for the specific task, which in our case is determining the "middle" of the shape. This dynamic weighting of neighbors can be particularly beneficial for handling irregular and complex geometric profiles.

The integration of attention mechanisms into GNNs has been shown to improve their performance on various graph-related tasks by enhancing their ability to capture intricate patterns and dependencies in the data 28. Furthermore, the successful application of attention-based shape matching using GNNs for generating virtual anatomical shapes suggests that this approach is effective in handling geometric relationships and could be well-suited for midcurve computation 29.

By allowing the model to selectively focus on the most informative parts of the input shape graph, attention mechanisms can lead to a more accurate and context-aware generation of the midcurve \[Insight 8.1\]. This is particularly true for complex shapes where different parts of the boundary might have varying degrees of influence on the medial structure. Attention can help the model learn these relationships and prioritize the relevant information during the generation process.

**9\. Representing the 1D Midcurve as a Graph: Design Considerations**

To effectively generate a 1D midcurve using graph neural networks, we need a suitable way to represent this curve as a graph 8. The choice of representation will influence the architecture of the decoder and the way we evaluate the generated output.

For a simple, non-branching midcurve, a natural representation is a **chain of connected nodes** 6. Each node in this chain corresponds to a point on the curve, and the edges represent the connections between consecutive points. The order of the nodes in the chain defines the connectivity of the curve.

If the midcurve exhibits branches, the graph representation will need to accommodate nodes with a **degree greater than 2** 26. These higher-degree nodes represent the points where the curve splits into multiple paths. The edges connected to these nodes will lead to the different branches of the midcurve.

For midcurves that contain loops, the graph representation will include **cycles** 26. A cycle occurs when there is a path in the graph that starts and ends at the same node, indicating a closed loop in the curve.

In addition to the topological structure, the graph representation must also capture the **geometric shape** of the midcurve. This is primarily achieved through the features associated with the nodes and edges. The most crucial node features will likely be the **spatial coordinates (x, y)** of the points on the midcurve \[User Query\]. Depending on the desired level of detail, other features like tangent vectors or curvature at each point could also be included. The edges connecting the nodes can have features representing the **length of the segment** between the two connected points (calculated using the Euclidean distance) 6. If the midcurve is not strictly a polyline but involves curved segments, the edge features might need to encode parameters that define the shape of these curve segments.

The choice of graph representation for the 1D midcurve needs to be sufficiently flexible to capture its potential topological complexity, including branches and loops, as well as its geometric shape, defined by the coordinates of the constituent points and the properties of the connecting segments \[Insight 9.1\]. The selection of appropriate node and edge features will be critical for accurately representing the geometric details of the midcurve.

**10\. Potential Model Architectures and Training Strategies**

Based on the reviewed research, several encoder-decoder architectures built upon Graph Neural Networks (GNNs) could be adapted for the midcurve computation problem \[Insight 10.1\].

One promising architecture is a **Graph Convolutional Encoder \- Autoregressive Graph Decoder**. In this setup, multiple layers of GCNs (or other suitable GNNs like GraphSAGE) would encode the 2D profile graph into a fixed-length latent vector. The decoder would then be an autoregressive model, possibly based on an RNN or a Transformer, that generates the 1D midcurve graph sequentially. At each step, the decoder could decide to add a new node with specific coordinates and connect it to existing nodes, conditioned on the latent embedding and the previously generated part of the midcurve. This approach has the potential to handle variable output sizes and complex topologies like branches and loops.

Another possibility is a **Graph Autoencoder with a Specialized 1D Graph Decoder**. Here, the encoder maps the 2D profile to a latent space, and the decoder is specifically designed to generate a 1D graph. This decoder might predict the adjacency matrix and node coordinates of the midcurve, possibly incorporating mechanisms to ensure the resulting graph has a 1D structure and can handle non-linear features.

An **Attention-based Graph Encoder-Decoder** could also be highly effective. By incorporating attention mechanisms in both the encoder and decoder, the model can learn to focus on the most relevant parts of the input shape when generating the midcurve. The encoder could use attention to identify important features in the 2D profile, while the decoder could use attention to decide which aspects of the encoded information are most relevant at each step of the midcurve generation. This architecture could be particularly beneficial for handling complex input shapes.

Regardless of the specific architecture, a **supervised training dataset** consisting of pairs of 2D profile graphs and their corresponding 1D midcurve graphs is essential \[User Query\]. The model will learn the mapping between these pairs during the training process.

The choice of **loss function** will be critical for guiding the training. A **reconstruction loss** that measures the difference between the generated midcurve graph and the ground truth midcurve graph is necessary. This loss could compare the positions of the nodes and the connectivity of the edges. For example, a combination of a point-wise distance loss and a loss on the adjacency matrix could be used. Additionally, a **geometric constraints loss** could be incorporated to encourage the generated midcurve to lie "in the middle" of the 2D profile. This might involve defining a suitable distance metric or using techniques like signed distance fields. If preserving the topology of the midcurve (e.g., the number of branches or loops) is important, additional **topological loss** terms could be considered.

The selection of the most appropriate model architecture and training strategy will depend on the specific characteristics of the 2D profiles and their midcurves in the available training dataset. A supervised learning approach with a carefully designed loss function that considers both geometric accuracy and topological correctness is likely to be the most effective way to train a model for this task.

**Conclusions**

The analysis indicates that employing graph-based deep learning, particularly through encoder-decoder architectures, offers a promising direction for tackling the midcurve computation problem. The inherent ability of Graph Neural Networks to process non-Euclidean data like geometric shapes represented as graphs makes them well-suited for this task. Addressing the challenges of variable input and output graph sizes is crucial, and techniques like sequential generation in the decoder appear to be a viable solution. Incorporating geometric information through geometric deep learning is essential for ensuring the generated midcurve accurately reflects the spatial characteristics of the input profile. Furthermore, the ability to generate non-linear structures such as loops and branches, often found in midcurves of complex shapes, can potentially be achieved through sequential or autoregressive generation processes. Attention mechanisms within GNNs can enhance the model's performance by allowing it to focus on the most relevant parts of the input shape. A supervised learning approach, leveraging the available dataset of 2D profiles and their corresponding midcurves, and employing a carefully designed loss function that considers both geometric and topological accuracy, will be key to training an effective model for midcurve computation. The conceptual similarity to graph summarization also suggests that techniques from that field could provide valuable insights and starting points for developing a solution.

#### **Works cited**

1. Geometry, Graphs and GPT. Driving Geometric Transformation… | by Yogesh Haribhau Kulkarni (PhD) | Technology Hits | Medium, accessed March 27, 2025, [https://medium.com/technology-hits/geometry-graphs-and-gpt-2862d6d24866](https://medium.com/technology-hits/geometry-graphs-and-gpt-2862d6d24866)  
2. Graph Neural Network Variable Input Size \- Restack, accessed March 27, 2025, [https://www.restack.io/p/neural-networks-answer-graph-neural-network-variable-input-size-cat-ai](https://www.restack.io/p/neural-networks-answer-graph-neural-network-variable-input-size-cat-ai)  
3. 10.6. The Encoder–Decoder Architecture — Dive into Deep Learning 1.0.3 documentation, accessed March 27, 2025, [https://d2l.ai/chapter\_recurrent-modern/encoder-decoder.html](https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html)  
4. 9.7. Sequence to Sequence Learning — Dive into Deep Learning 0.17.6 documentation, accessed March 27, 2025, [https://classic.d2l.ai/chapter\_recurrent-modern/seq2seq.html](https://classic.d2l.ai/chapter_recurrent-modern/seq2seq.html)  
5. Encoder-Decoder Long Short-Term Memory Networks \- MachineLearningMastery.com, accessed March 27, 2025, [https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/](https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/)  
6. Graph Neural Network and Some of GNN Applications: Everything You Need to Know, accessed March 27, 2025, [https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications](https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications)  
7. A Comprehensive Introduction to Graph Neural Networks (GNNs ..., accessed March 27, 2025, [https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial](https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial)  
8. Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and Future \- PMC \- PubMed Central, accessed March 27, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC8309939/](https://pmc.ncbi.nlm.nih.gov/articles/PMC8309939/)  
9. GRAPH NEURAL NETWORKS : The Brainchild of Graph Theory and Deep Learning | by Abdelhedi Youssef | Medium, accessed March 27, 2025, [https://medium.com/@abdelhedi.youssef8/graph-neural-networks-the-brainchild-of-graph-theory-and-deep-learning-3c83d6a233d7](https://medium.com/@abdelhedi.youssef8/graph-neural-networks-the-brainchild-of-graph-theory-and-deep-learning-3c83d6a233d7)  
10. graph-neural-networks.github.io, accessed March 27, 2025, [https://graph-neural-networks.github.io/static/file/chapter11.pdf](https://graph-neural-networks.github.io/static/file/chapter11.pdf)  
11. Graph Attention Networks: A Comprehensive Review of Methods and Applications \- MDPI, accessed March 27, 2025, [https://www.mdpi.com/1999-5903/16/9/318](https://www.mdpi.com/1999-5903/16/9/318)  
12. (PDF) Graphs Unveiled: Graph Neural Networks and Graph Generation \- ResearchGate, accessed March 27, 2025, [https://www.researchgate.net/publication/378846405\_Graphs\_Unveiled\_Graph\_Neural\_Networks\_and\_Graph\_Generation](https://www.researchgate.net/publication/378846405_Graphs_Unveiled_Graph_Neural_Networks_and_Graph_Generation)  
13. Improving Graph Machine Learning Performance Through Feature Augmentation Based on Network Control Theory \- arXiv, accessed March 27, 2025, [https://arxiv.org/html/2405.03706v1](https://arxiv.org/html/2405.03706v1)  
14. Graph Autoencoder of arbitrary node size, how to decode? \- Data Science Stack Exchange, accessed March 27, 2025, [https://datascience.stackexchange.com/questions/130996/graph-autoencoder-of-arbitrary-node-size-how-to-decode](https://datascience.stackexchange.com/questions/130996/graph-autoencoder-of-arbitrary-node-size-how-to-decode)  
15. \[R\] Graph Autoencoder of arbitrary node size, how to decode? : r/MachineLearning \- Reddit, accessed March 27, 2025, [https://www.reddit.com/r/MachineLearning/comments/1hk7lje/r\_graph\_autoencoder\_of\_arbitrary\_node\_size\_how\_to/](https://www.reddit.com/r/MachineLearning/comments/1hk7lje/r_graph_autoencoder_of_arbitrary_node_size_how_to/)  
16. Learning Deep Generative Models of Graphs, accessed March 27, 2025, [http://www.cs.toronto.edu/\~yujiali/files/talks/Learning\_deep\_graph\_generative\_models.pdf](http://www.cs.toronto.edu/~yujiali/files/talks/Learning_deep_graph_generative_models.pdf)  
17. Graph Attention with Random Rewiring \- arXiv, accessed March 27, 2025, [https://arxiv.org/html/2407.05649v1](https://arxiv.org/html/2407.05649v1)  
18. Graph Summarization with Quality Guarantees \- Two Sigma, accessed March 27, 2025, [https://www.twosigma.com/wp-content/uploads/Riondato\_-\_GraphSummarization\_-\_DMKD\_-\_Paper.pdf](https://www.twosigma.com/wp-content/uploads/Riondato_-_GraphSummarization_-_DMKD_-_Paper.pdf)  
19. A Comprehensive Survey on Graph Summarization with Graph Neural Networks \- arXiv, accessed March 27, 2025, [http://arxiv.org/pdf/2302.06114](http://arxiv.org/pdf/2302.06114)  
20. A Comprehensive Survey on Graph Summarization With Graph Neural Networks, accessed March 27, 2025, [https://www.computer.org/csdl/journal/ai/2024/08/10382477/1TxRQYbrFTO](https://www.computer.org/csdl/journal/ai/2024/08/10382477/1TxRQYbrFTO)  
21. A Comprehensive Survey on Graph Summarization With Graph Neural Networks, accessed March 27, 2025, [https://www.researchgate.net/publication/377250542\_A\_Comprehensive\_Survey\_on\_Graph\_Summarization\_with\_Graph\_Neural\_Networks](https://www.researchgate.net/publication/377250542_A_Comprehensive_Survey_on_Graph_Summarization_with_Graph_Neural_Networks)  
22. Introduction to Geometric Deep Learning | ml-articles – Weights & Biases \- Wandb, accessed March 27, 2025, [https://wandb.ai/mostafaibrahim17/ml-articles/reports/Introduction-to-Geometric-Deep-Learning--VmlldzozODY5NTE1](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Introduction-to-Geometric-Deep-Learning--VmlldzozODY5NTE1)  
23. Towards Geometric Deep Learning \- The Gradient, accessed March 27, 2025, [https://thegradient.pub/towards-geometric-deep-learning/](https://thegradient.pub/towards-geometric-deep-learning/)  
24. Graph Neural Networks for 3D Shape Analysis \- Drexel Research Discovery, accessed March 27, 2025, [https://researchdiscovery.drexel.edu/esploro/outputs/doctoral/Graph-Neural-Networks-for-3D-Shape/991021890111904721](https://researchdiscovery.drexel.edu/esploro/outputs/doctoral/Graph-Neural-Networks-for-3D-Shape/991021890111904721)  
25. Graph Attention Networks. Graph Machine Learning | by Ashish Kumar | Medium, accessed March 27, 2025, [https://medium.com/@ashish28082002.ak/graph-attention-networks-52f03591b3cc](https://medium.com/@ashish28082002.ak/graph-attention-networks-52f03591b3cc)  
26. Branches and loops \- LlamaIndex, accessed March 27, 2025, [https://docs.llamaindex.ai/en/stable/understanding/workflows/branches\_and\_loops/](https://docs.llamaindex.ai/en/stable/understanding/workflows/branches_and_loops/)  
27. How-to Guides \- GitHub Pages, accessed March 27, 2025, [https://langchain-ai.github.io/langgraph/how-tos/](https://langchain-ai.github.io/langgraph/how-tos/)  
28. Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks, accessed March 27, 2025, [https://arxiv.org/html/2409.03463v2](https://arxiv.org/html/2409.03463v2)  
29. A Geometric Deep Learning Framework for Generation of Virtual Left Ventricles as Graphs, accessed March 27, 2025, [https://proceedings.mlr.press/v227/kalaie24a.html](https://proceedings.mlr.press/v227/kalaie24a.html)


---
# Implementation

Prompt: "Any Open-source implementation of the above problem?"

Response:

### **Open-Source Implementations of Midcurve Computation Using Graph Neural Networks**

**1\. Introduction: The Significance of Midcurve Computation in Shape Analysis**

Midcurve computation, a process also referred to as shape skeletonization or, in certain contexts, medial axis transform, involves the extraction of a lower-dimensional representation, typically a one-dimensional curve or a graph structure, that effectively captures the fundamental structural and topological characteristics of a given two-dimensional or three-dimensional shape. The intuitive concept behind a midcurve is that it resides centrally within the shape, maintaining an equal distance from its boundaries 1. This central representation serves as a simplified yet informative abstraction of the original shape, facilitating various downstream analytical and processing tasks.

The importance of midcurve computation spans a multitude of applications across diverse scientific and engineering disciplines. In the realm of shape analysis and comparison, midcurves provide a concise descriptor that allows for efficient quantification of shape similarity and dissimilarity. For shape matching and retrieval tasks, the structural information encoded in a midcurve can be a robust feature, invariant to certain transformations and deformations. Mesh simplification and abstraction benefit from midcurves by using them as a guide to reduce the complexity of a shape's representation while preserving its essential form. Furthermore, in path planning and navigation, particularly in indoor environments, the medial axis transform, a close relative of midcurve computation, can be utilized to determine optimal routes 2. The field of computer animation and character rigging leverages skeletal representations, derived from midcurve-like structures, to articulate and deform virtual characters realistically 3. Medical image analysis also employs these techniques, for instance, in the generation of virtual anatomical models such as the left ventricle 4. Even in manufacturing and CAD/CAM applications, the concept of a midsurface, analogous to a midcurve in three dimensions for thin-walled objects, is crucial for finite element analysis and other engineering computations 5.

In recent years, graph neural networks (GNNs) have emerged as a powerful class of deep learning models specifically designed to process data structured as graphs. Given that the connectivity and geometry of shapes can be effectively represented using graph structures, GNNs hold significant potential for analyzing, generating, and manipulating these representations. This report aims to explore the current landscape of open-source implementations that utilize graph neural networks for the task of midcurve computation, drawing upon a predefined search strategy and the provided collection of research materials.

The widespread applicability of midcurve computation across numerous domains, combined with the inherent capability of graph neural networks to handle the complex and non-Euclidean nature of shape data, positions this intersection as a critical area of investigation. The detailed search strategy employed by the user underscores a focused and informed interest in identifying existing, readily usable resources within this domain. The expectation is that this report will provide specific pointers to code repositories and research that directly address or are closely related to the computation of midcurves using graph neural networks.

**2\. Representing Shapes as Graphs for Graph Neural Networks**

The efficacy of applying graph neural networks to shape analysis tasks, including midcurve computation, fundamentally depends on how effectively two-dimensional and three-dimensional shapes can be represented as graph structures. Several methods exist for this conversion, each with its own strengths and weaknesses in capturing different aspects of a shape's geometry and topology.

One common approach involves treating a shape as a collection of points in space, known as a point cloud. In this representation, each individual point on the shape's surface or interior becomes a node in the graph. Edges between these nodes are then defined based on various criteria, such as the spatial proximity between points (e.g., connecting each point to its k-nearest neighbors) or other geometric relationships, such as the orientation of surface normals 6. For instance, a dataset of 3D models might be provided as a set of point clouds 7. The process of converting a point cloud to a graph often involves determining these neighborhood relationships to establish the edges.

Another prevalent representation for three-dimensional shapes is the polygonal mesh. In this case, the vertices of the mesh are naturally represented as the nodes of the graph, and the edges of the mesh (connecting adjacent vertices) directly translate to the edges in the graph 8. Furthermore, some approaches extend this basic structure by incorporating information about the faces of the mesh, potentially as additional features associated with the nodes or edges, allowing the GNN to leverage more detailed geometric context 8. The analysis of 3D shapes represented as meshes using graph neural networks has been a significant area of research.

Volumetric data, where a shape is represented by a three-dimensional grid of voxels, can also be converted into a graph. In this scenario, each voxel occupies a node in the graph, and edges are typically established between neighboring voxels in the grid structure 6. This representation is particularly useful for analyzing solid objects or regions of space occupied by a shape.

Finally, if a preliminary skeletal representation or medial axis of the shape is already available, its constituent elements can form the basis of a graph. For example, the points or segments that make up the skeleton can be treated as nodes, with edges representing their connectivity along the skeletal structure.

Beyond the basic graph structure, the success of GNNs in shape analysis heavily relies on the features associated with the nodes and edges. These features encode relevant geometric information that the network can learn from. Examples of such features include the three-dimensional coordinates of the vertices, the orientation of surface normals at each point, local curvatures of the shape, and distances between neighboring points or vertices 9. The careful design and selection of these features are crucial for enabling the GNN to understand the underlying geometry of the shape and perform tasks like midcurve computation effectively.

A significant challenge in applying GNNs to shape analysis arises from the inherent variability in the topology and resolution of different shapes 11. Shapes can have different numbers of connected components, holes, and levels of detail in their geometric representation. Constructing consistent graph representations that can effectively handle this diversity is an ongoing area of research. The chosen method for converting a shape into a graph significantly impacts the information that is captured and, consequently, the performance of any subsequent GNN models applied to that graph. Different representations emphasize different aspects of the shape, such as its surface characteristics versus its volumetric occupancy, and they also come with varying computational costs for processing. Therefore, the architecture of the graph neural network needs to be carefully tailored to the specific graph representation chosen for the shape data. The inherent variability in the complexity of shapes necessitates the development of robust and adaptable representation methods that can effectively capture the essential geometric and topological information regardless of the shape's specific characteristics.

**3\. Open-Source Implementations for Shape Skeletonization using Deep Learning (Related to Midcurve Computation)**

An analysis of the provided research materials reveals several open-source projects related to shape skeletonization using deep learning, a task closely aligned with midcurve computation. While not all of these projects directly employ graph neural networks, their methodologies and codebases offer valuable insights and potential resources for addressing the user's query.

**3.1. Coverage Axis++:**

The research snippets 12 introduce Coverage Axis++, a novel and efficient algorithm specifically designed for three-dimensional shape skeletonization. This method aims to approximate the Medial Axis Transform (MAT) of a shape. Notably, the code for Coverage Axis++ is publicly available on GitHub at the repository https://github.com/Frank-ZY-Dou/Coverage\_Axis 13. A key feature of Coverage Axis++ is its ability to handle various representations of 3D shapes, including polygonal meshes, point clouds, and even triangle soups. Furthermore, the algorithm allows users to specify the desired number of skeletal points in the output. While Coverage Axis++ itself does not directly utilize graph neural networks, its primary objective of efficiently extracting a shape's skeleton makes it highly relevant to the problem of midcurve computation. It could potentially serve as a valuable baseline for comparison with GNN-based methods, or its output could be used as a target for a GNN to learn to predict. Moreover, the underlying principles of Coverage Axis++, which involve selecting a minimal set of inner points to "cover" the shape, might inspire the development of novel graph-based approaches for midcurve extraction. The existence of such an efficient and versatile open-source tool for skeletonization suggests an active community interest in this domain, even if GNNs are not always the chosen methodology. This provides a valuable resource for the user, potentially for direct use, comparison, or as a component in a hybrid approach.

**3.2. L1-Medial Skeleton:**

Another significant open-source project identified in the research materials 17 is the L1-Medial Skeleton algorithm. This method focuses on extracting curve skeletons specifically from three-dimensional point clouds and is known for its robustness to noise, outliers, and even significant amounts of missing data in the input point cloud. A Python implementation of the L1-Medial Skeleton algorithm is available on GitHub at https://github.com/SmartPolarBear/l1skeleton\_py 22. Similar to Coverage Axis++, the L1-Medial Skeleton algorithm, in its standard form, does not directly employ graph neural networks. However, its core concept of locally adapting L1-medians to identify a central curve within a point cloud could potentially be extended or adapted for two-dimensional shapes or for processing graph representations of shapes. The availability of a robust open-source algorithm like the L1-Medial Skeleton highlights the complexity of the skeletonization problem and the diverse approaches being explored within the research community. This implementation could serve as another valuable resource for the user, either for direct application to point cloud data, for comparison with potential GNN-based methods, or as a source of inspiration for developing new techniques.

**3.3. GEM3D:**

The research snippets 32 introduce GEM3D, a novel neural generative model for three-dimensional shapes. A key aspect of GEM3D is its use of a skeleton-based representation that follows the Medial Axis Transform (MAT). This model demonstrates the potential of deep learning to directly generate skeleton-like representations, referred to as medial abstractions, which is highly relevant to the task of midcurve computation. While the snippets discuss the architecture and capabilities of GEM3D, they do not explicitly state whether the code for this project is publicly available. The primary focus of GEM3D is on the generation of both the skeletal representation and the corresponding 3D surface. The research on generative models like GEM3D signifies a growing trend towards utilizing neural networks to directly learn and generate shape skeletons or midcurves, moving beyond traditional geometric methodologies. If the code for GEM3D were to become publicly accessible, it would represent a highly valuable resource for researchers interested in GNN-based midcurve computation.

**3.4. P2MAT-NET:**

Another relevant research project is P2MAT-NET, presented in 34. This project introduces a neural network designed to learn the transformation from sparse three-dimensional point clouds into a set of spheres that approximate the Medial Axis Transform (MAT). While the snippet details the network architecture and its performance, it does not explicitly mention the open-source availability of the code. P2MAT-NET illustrates a deep learning approach for predicting a sphere-based representation of the medial axis from point clouds, which can be considered a significant step towards shape skeletonization. This work demonstrates the use of deep learning techniques to learn intermediate representations like the MAT, which can then potentially be processed further to obtain a more traditional skeletal or midcurve representation.

**Table 1: Summary of Relevant Open-Source Skeletonization Projects**

| Project Name | Primary Task | Input Data Type | Deep Learning (Yes/No) | GitHub Link | Key Features/Notes | Relevance to Midcurve Computation |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Coverage Axis++ | 3D Shape Skeletonization | Meshes, Point Clouds, Soups | No | https://github.com/Frank-ZY-Dou/Coverage\_Axis | Efficient, approximates MAT, tunable number of points | Provides a strong baseline and potential inspiration for GNN-based methods. |
| l1skeleton\_py | Curve Skeleton Extraction | 3D Point Clouds | No | https://github.com/SmartPolarBear/l1skeleton\_py | Robust to noise and missing data, Python implementation | Offers another non-GNN approach that could be adapted or compared against. |

**4\. Graph Encoder-Decoder Architectures for Shape Representation and Midcurve Extraction**

Graph encoder-decoder architectures represent a powerful paradigm in deep learning for processing graph-structured data, and they hold significant potential for both shape representation and midcurve extraction 35. The fundamental concept involves two main components: an encoder and a decoder. The encoder takes an input graph, which in this context would represent a shape (e.g., a point cloud graph or a mesh graph), and maps it into a lower-dimensional latent space representation. This latent representation aims to capture the essential features and characteristics of the input shape in a compressed form. Subsequently, the decoder takes this latent space embedding as input and attempts to reconstruct the original shape or generate a related output. In the context of midcurve computation, the desired output from the decoder would ideally be a graph representing the midcurve of the input shape.

A specific type of encoder-decoder architecture, the graph autoencoder 40, is particularly relevant. Graph autoencoders learn to encode the input graph into a lower-dimensional embedding and then decode it back to reconstruct the original graph structure. The bottleneck layer in the middle of the autoencoder architecture forces the model to learn the most salient features of the input data in order to achieve successful reconstruction. It is hypothesized that these salient features might implicitly encode information about the shape's central structure, which could be leveraged for midcurve computation.

Despite the conceptual promise of graph encoder-decoder architectures for this task, a direct search through the provided research snippets reveals a lack of readily available open-source implementations specifically designed for midcurve extraction using graph neural networks. While 1 explicitly frames midcurve generation as an encoder-decoder problem where a larger input graph (representing a polygon) is reduced to a smaller output graph (a polyline), it does not point to a specific open-source implementation that utilizes GNNs for this purpose. Similarly37 and 38 discuss deep encoder-decoder neural network architectures for processing graph signals, which suggests that this is an active area of research within the broader field of geometric deep learning. However, these discussions do not provide specific open-source code tailored for midcurve computation of geometric shapes.

One of the key challenges in utilizing graph encoder-decoder architectures for midcurve computation lies in the fact that the output, which is the midcurve graph, can have a variable size depending on the complexity of the input shape 42. The number of nodes and edges in the midcurve graph will generally vary across different shapes. This poses a problem for the decoder, which typically expects to generate an output of a fixed size or structure. Techniques to address this challenge might involve predicting the number of nodes and edges in the midcurve as part of the decoding process, or employing sequential generation approaches where the midcurve graph is built node by node or edge by edge.

In this context, the relevance of sequence-to-sequence models and their adaptation to graph data should also be considered 44. Sequence-to-sequence models are designed to handle input and output sequences of variable lengths, making them potentially suitable for generating a midcurve graph with a varying number of components from a shape graph. Adapting these models to operate on graph structures could involve processing the graph in a sequential manner or using attention mechanisms to focus on different parts of the graph during the generation process.

While direct open-source implementations of graph encoder-decoder architectures for midcurve computation might be currently limited, the underlying concepts and the ongoing research in this area suggest a promising avenue for future development. The ability of these architectures to learn compressed shape representations and potentially generate related graph structures makes them a strong candidate for tackling the problem of midcurve extraction.

**5\. Graph Summarization Techniques for Geometric Dimension Reduction (Alternative Approaches)**

Graph summarization techniques offer an alternative perspective on achieving geometric dimension reduction, which could potentially be adapted for obtaining a midcurve-like representation of a shape 35. The primary goal of graph summarization is to find a condensed representation of a large and complex graph while preserving its most important structural properties.

If a shape is represented as a dense graph, where nodes correspond to points or vertices and edges represent connections based on proximity or adjacency, graph summarization techniques could potentially be employed to extract a sparser subgraph that captures the essential structure of the shape, resembling a midcurve. For instance48 explores the inherent connections between dimensionality reduction techniques and graph drawing algorithms, suggesting that methods from graph summarization could be used to model relationships within high-dimensional geometric data. Similarly50 discusses graph summarization by partitioning the original set of vertices into a smaller number of supernodes connected by superedges, where the weights of these superedges represent the edge densities between the vertices in the corresponding supernodes. This process of aggregation and simplification could potentially lead to a lower-resolution representation of a shape graph that retains its central topological structure, akin to a midcurve.

Notably52 presents research on "Graph Summarization with Graph Neural Networks," indicating that GNNs are indeed being used for graph summarization tasks. In this particular work, the focus is on preserving predefined features of a graph's vertices within a local neighborhood in the context of large web graphs, rather than directly addressing geometric shapes. However, it demonstrates the applicability of GNNs to the task of graph simplification and abstraction.

It is important to note that while graph summarization techniques aim to reduce the complexity of graphs, their direct applicability to extracting a geometrically accurate midcurve of a shape might necessitate careful adaptation. Standard graph summarization methods might focus on preserving properties like connectivity patterns or node attributes, and they might not inherently optimize for the geometric fidelity required in a midcurve representation. Therefore, the design of appropriate summarization objectives and the use of loss functions that specifically encourage the preservation of the shape's central curve would be crucial for adapting these techniques to the task of midcurve computation.

**6\. Academic Papers on Midcurve Computation or Shape Skeletonization with Open-Source Code**

A re-examination of the research snippets for academic papers that explicitly provide links to their open-source code or implementations related to midcurve computation or shape skeletonization using GNNs reveals that while several relevant methods are discussed, direct code links within the abstract or main text are not consistently provided. Many papers, such as those introducing GEM3D 32 and P2MAT-NET 34, describe their methodologies and results in detail but do not immediately offer links to their code.

The most promising leads for open-source code identified thus far are for the non-GNN methods: Coverage Axis++ 13 and l1skeleton\_py (for the L1-Medial Skeleton algorithm) 22. These projects have publicly accessible GitHub repositories.

It is crucial for the user to recognize the importance of checking the full text of the identified papers, as well as any supplementary materials that might be provided alongside the publications. Often, researchers include links to their code repositories or project websites in these additional resources.

Furthermore, platforms like Papers with Code ([https://paperswithcode.com/](https://paperswithcode.com/)) can be an invaluable tool for discovering open-source implementations associated with specific academic papers. As demonstrated by 15, Coverage Axis++ is listed on Papers with Code, which provides a direct link to its GitHub repository. Therefore, the user is strongly encouraged to utilize such platforms to search for implementations of the papers mentioned in the snippets and other relevant publications found through their own literature review. While the provided snippets offer references to pertinent research, actively seeking out associated code on platforms like GitHub and Papers with Code is a necessary step in the user's pursuit of open-source implementations for midcurve computation using graph neural networks.

**7\. Exploration of Deep Learning Frameworks and Libraries**

Even in the absence of direct open-source implementations of midcurve computation using graph neural networks, the robust and versatile nature of modern deep learning frameworks like TensorFlow and PyTorch, along with their associated libraries, provides researchers with the necessary tools to build their own custom GNN models for this task.

PyTorch Geometric (PyG) is a prime example of such a library. It offers a rich set of functionalities specifically designed for implementing graph neural networks 9. PyG includes implementations of various graph convolution layers, pooling mechanisms for hierarchical graph processing, and a wide array of utility functions for handling graph data structures efficiently. Similarly, the Deep Graph Library (DGL) is another popular and powerful framework that supports the efficient implementation of GNNs. DGL provides a flexible platform for defining custom graph structures and message passing schemes, enabling researchers to tailor their GNN models to the specific requirements of their tasks.

The availability of these comprehensive frameworks and libraries means that even if direct, ready-to-use open-source implementations of GNNs for midcurve computation are not readily discoverable, the user can leverage the provided research papers and the functionalities offered by PyTorch Geometric and DGL to implement their own models. For instance, if a research paper proposes a specific graph encoder-decoder architecture or utilizes attention mechanisms within a GNN for a related shape analysis task, the user can utilize these libraries to build and train a model based on those architectural principles.

Moreover, both PyTorch Geometric and DGL are accompanied by extensive documentation that includes numerous tutorials and examples. These resources can be invaluable for users who are new to implementing GNNs for geometric data or who are looking for guidance on specific aspects of model development. By exploring these tutorials and examples, the user can gain a practical understanding of how to work with graph data, implement different GNN layers, and train models for various graph-based tasks, providing a strong foundation for tackling the problem of midcurve computation.

**8\. Communities and Forums for Geometric Modeling and Graph Neural Networks**

Exploring online communities and forums dedicated to geometric modeling and graph neural networks can be a valuable strategy for discovering open-source code or projects related to midcurve computation that might not be easily found through standard search methods.

GitHub, beyond being a platform for hosting code repositories, also serves as a community hub for developers and researchers. By exploring organizations or individual user profiles that specialize in geometric deep learning or shape analysis, the user might uncover relevant projects that are not immediately apparent through keyword searches.

Platforms like Reddit, particularly subreddits such as r/MachineLearning, r/deeplearning, and r/geometricdeeplearning, often host discussions where researchers share links to recent papers and associated code implementations. Engaging with these communities by searching for relevant keywords or posting specific questions about midcurve computation using GNNs could potentially lead to valuable discoveries.

Specialized forums focused on computer graphics, computational geometry, or specific deep learning frameworks (e.g., forums dedicated to PyTorch or TensorFlow) might also contain relevant discussions or shared resources related to the user's query.

Furthermore, the websites of major conferences in the fields of machine learning, computer vision, and computer graphics (e.g., NeurIPS, ICML, CVPR, SIGGRAPH) often provide links to accepted papers, and sometimes these links extend to the authors' code repositories. Additionally, workshops related to geometric deep learning, frequently held in conjunction with these conferences, can be valuable sources of information and potential code sharing.

Actively engaging with these communities by asking specific questions about the challenges and solutions related to midcurve computation using GNNs is highly recommended. Researchers in these forums might be aware of less visible or newly released open-source projects that directly address the user's needs or could offer valuable insights and guidance based on their own work in this area.

**9\. Tutorials and Blog Posts on Applying GNNs to Shape Analysis**

Searching for tutorials and blog posts that provide code examples for applying graph neural networks to shape analysis or skeletonization tasks can be a fruitful avenue for gaining practical knowledge and potentially finding relevant code snippets. Platforms such as Medium, Towards Data Science, and personal research blogs often feature articles that explain complex techniques in a more accessible manner and sometimes include accompanying code implementations.

The official documentation of libraries like PyTorch Geometric and Deep Graph Library often includes tutorials that demonstrate how to apply GNNs to various types of geometric data, such as point clouds and meshes. While these tutorials might not directly address midcurve computation, they can provide valuable building blocks and code examples for implementing the necessary GNN architectures and data processing pipelines. For instance, a tutorial on graph-based point cloud classification or mesh segmentation might contain code for constructing graph representations of shapes and implementing graph convolution layers, which could be adapted for a midcurve computation task.

Even if these resources do not directly tackle the problem of midcurve computation, they can offer practical guidance on how to handle geometric data with GNNs, how to implement specific network layers or architectures, and how to train and evaluate these models. This foundational knowledge and the accompanying code examples can be instrumental in helping the user develop their own GNN-based solution for midcurve extraction.

**10\. Conclusion and Future Directions**

In summary, while a direct, readily available open-source implementation of midcurve computation using graph neural networks might not be prevalent in the current research landscape, the exploration of related fields and resources has yielded valuable findings. Open-source projects like Coverage Axis++ and l1skeleton\_py offer robust non-GNN solutions for shape skeletonization, a task closely related to midcurve computation, and can serve as valuable baselines or components in a hybrid approach. The concept of graph encoder-decoder architectures holds significant promise for learning shape representations and potentially generating midcurves, but direct open-source implementations for this specific application appear to be limited at present. Similarly, graph summarization techniques offer an alternative perspective on geometric dimension reduction, but their direct adaptation for geometrically accurate midcurve extraction requires further investigation.

The academic literature provides numerous relevant research papers, but direct links to open-source code within the snippets are not common. Actively searching platforms like GitHub and Papers with Code is crucial for uncovering available implementations. Deep learning frameworks such as TensorFlow and PyTorch, along with their specialized libraries like PyTorch Geometric and Deep Graph Library, provide the essential tools and resources for researchers to develop their own custom GNN models for midcurve computation based on existing research. Engaging with online communities and exploring tutorials and blog posts on applying GNNs to shape analysis can further facilitate the discovery of relevant code and practical insights.

Looking towards the future, several potential research directions could advance the field of midcurve computation using graph neural networks. Developing novel GNN architectures specifically tailored for midcurve extraction from various shape representations (point clouds, meshes, etc.) is a key area. Exploring the integration of attention mechanisms within these GNNs could enhance their ability to capture long-range dependencies in shape structure, leading to more accurate midcurve computation. Investigating specialized loss functions that directly encourage the generated graph to be a geometrically accurate and topologically correct midcurve is also crucial. The creation of benchmark datasets and standardized evaluation metrics specifically for midcurve computation using graph neural networks would help to drive progress and facilitate comparisons between different methods. Finally, adapting existing open-source skeletonization algorithms to incorporate the learning capabilities of GNNs could lead to improved robustness and accuracy in midcurve extraction.

Ultimately, while a direct, off-the-shelf open-source GNN solution for midcurve computation might not be immediately available, the user has access to a rich ecosystem of related research, powerful tools, and active communities. By leveraging the identified resources, engaging with the research community, and considering the development of their own GNN-based models, the user can contribute to and benefit from the ongoing advancements in this exciting and important area of geometric deep learning.

#### **Works cited**

1. Geometry, Graphs and GPT. Driving Geometric Transformation… | by Yogesh Haribhau Kulkarni (PhD) | Technology Hits | Medium, accessed March 27, 2025, [https://medium.com/technology-hits/geometry-graphs-and-gpt-2862d6d24866](https://medium.com/technology-hits/geometry-graphs-and-gpt-2862d6d24866)  
2. Automatic Generation of 3D Indoor Navigation Networks from ... \- MDPI, accessed March 27, 2025, [https://www.mdpi.com/2220-9964/12/6/231](https://www.mdpi.com/2220-9964/12/6/231)  
3. Predicting Animation Skeletons for 3D Articulated Models via ..., accessed March 27, 2025, [https://www.researchgate.net/publication/336952824\_Predicting\_Animation\_Skeletons\_for\_3D\_Articulated\_Models\_via\_Volumetric\_Nets](https://www.researchgate.net/publication/336952824_Predicting_Animation_Skeletons_for_3D_Articulated_Models_via_Volumetric_Nets)  
4. A Geometric Deep Learning Framework for Generation of Virtual Left Ventricles as Graphs, accessed March 27, 2025, [https://proceedings.mlr.press/v227/kalaie24a.html](https://proceedings.mlr.press/v227/kalaie24a.html)  
5. Extraction of generative processes from B-Rep shapes and application to idealization transformations | Request PDF \- ResearchGate, accessed March 27, 2025, [https://www.researchgate.net/publication/259085912\_Extraction\_of\_generative\_processes\_from\_B-Rep\_shapes\_and\_application\_to\_idealization\_transformations](https://www.researchgate.net/publication/259085912_Extraction_of_generative_processes_from_B-Rep_shapes_and_application_to_idealization_transformations)  
6. Graph representations of 3D data for machine learning \- arXiv, accessed March 27, 2025, [https://arxiv.org/html/2408.08336v1](https://arxiv.org/html/2408.08336v1)  
7. Introduction to Geometric Deep Learning | ml-articles – Weights & Biases \- Wandb, accessed March 27, 2025, [https://wandb.ai/mostafaibrahim17/ml-articles/reports/Introduction-to-Geometric-Deep-Learning--VmlldzozODY5NTE1](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Introduction-to-Geometric-Deep-Learning--VmlldzozODY5NTE1)  
8. Graph Neural Networks for 3D Shape Analysis \- Drexel Research Discovery, accessed March 27, 2025, [https://researchdiscovery.drexel.edu/esploro/outputs/doctoral/Graph-Neural-Networks-for-3D-Shape/991021890111904721](https://researchdiscovery.drexel.edu/esploro/outputs/doctoral/Graph-Neural-Networks-for-3D-Shape/991021890111904721)  
9. Graph Neural Network and Some of GNN Applications: Everything You Need to Know, accessed March 27, 2025, [https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications](https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications)  
10. (PDF) Component Segmentation of Engineering Drawings Using ..., accessed March 27, 2025, [https://www.researchgate.net/publication/365942886\_Component\_Segmentation\_of\_Engineering\_Drawings\_Using\_Graph\_Convolutional\_Networks](https://www.researchgate.net/publication/365942886_Component_Segmentation_of_Engineering_Drawings_Using_Graph_Convolutional_Networks)  
11. Foundation Models in Graph & Geometric Deep Learning | by Michael Galkin \- Medium, accessed March 27, 2025, [https://medium.com/towards-data-science/foundation-models-in-graph-geometric-deep-learning-f363e2576f58](https://medium.com/towards-data-science/foundation-models-in-graph-geometric-deep-learning-f363e2576f58)  
12. Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization \- arXiv, accessed March 27, 2025, [https://arxiv.org/html/2401.12946v2](https://arxiv.org/html/2401.12946v2)  
13. Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization, accessed March 27, 2025, [https://diglib.eg.org/bitstream/handle/10.1111/cgf15143/v43i5\_16\_cgf15143.pdf](https://diglib.eg.org/bitstream/handle/10.1111/cgf15143/v43i5_16_cgf15143.pdf)  
14. Frank-ZY-Dou/Coverage\_Axis: Official code for the paper ... \- GitHub, accessed March 27, 2025, [https://github.com/Frank-ZY-Dou/Coverage\_Axis](https://github.com/Frank-ZY-Dou/Coverage_Axis)  
15. Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization, accessed March 27, 2025, [https://paperswithcode.com/paper/coverage-axis-efficient-inner-point-selection](https://paperswithcode.com/paper/coverage-axis-efficient-inner-point-selection)  
16. (PDF) Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization, accessed March 27, 2025, [https://www.researchgate.net/publication/382714844\_Coverage\_Axis\_Efficient\_Inner\_Point\_Selection\_for\_3D\_Shape\_Skeletonization](https://www.researchgate.net/publication/382714844_Coverage_Axis_Efficient_Inner_Point_Selection_for_3D_Shape_Skeletonization)  
17. L-1-Medial Skeleton of Point Cloud | Request PDF \- ResearchGate, accessed March 27, 2025, [https://www.researchgate.net/publication/262295298\_L-1-Medial\_Skeleton\_of\_Point\_Cloud](https://www.researchgate.net/publication/262295298_L-1-Medial_Skeleton_of_Point_Cloud)  
18. L1-Medial Skeleton of Point Cloud \- SciSpace, accessed March 27, 2025, [https://scispace.com/pdf/l1-medial-skeleton-of-point-cloud-3p0197c61b.pdf](https://scispace.com/pdf/l1-medial-skeleton-of-point-cloud-3p0197c61b.pdf)  
19. L1-Medial Skeleton of Point Cloud \- School of Computing Science, accessed March 27, 2025, [https://www.cs.sfu.ca/\~haoz/pubs/huang\_sig13\_l1skel.pdf](https://www.cs.sfu.ca/~haoz/pubs/huang_sig13_l1skel.pdf)  
20. L1-Medial Skeleton of Point Cloud | Request PDF \- ResearchGate, accessed March 27, 2025, [https://www.researchgate.net/publication/261562372\_L1-Medial\_Skeleton\_of\_Point\_Cloud](https://www.researchgate.net/publication/261562372_L1-Medial_Skeleton_of_Point_Cloud)  
21. L1-Medial Skeleton of Point Cloud \- VCC, accessed March 27, 2025, [https://vcc.tech/research/2013/L1skeleton](https://vcc.tech/research/2013/L1skeleton)  
22. SmartPolarBear/l1skeleton\_py \- GitHub, accessed March 27, 2025, [https://github.com/SmartPolarBear/l1skeleton\_py](https://github.com/SmartPolarBear/l1skeleton_py)  
23. Maswor/PointCloud-Skeletonization: Algorithm to extract the visual skeleton of the scanned objects. Mirror from the research group's private repo. \- GitHub, accessed March 27, 2025, [https://github.com/Maswor/PointCloud-Skeletonization](https://github.com/Maswor/PointCloud-Skeletonization)  
24. Lixiao-bai/pc\_skeletor: Skeletonization of 3D Point Clouds \- GitHub, accessed March 27, 2025, [https://github.com/Lixiao-bai/pc\_skeletor](https://github.com/Lixiao-bai/pc_skeletor)  
25. GLSkeleton: A Geometric Laplacian-based Skeletonisation Framework for Object Point Clouds \- ORCA – Online Research @ Cardiff, accessed March 27, 2025, [https://orca.cardiff.ac.uk/id/eprint/167660/1/RAL\_ICRA\_2024\_Qingmeng\_GLSkeleton\_Final%283%29.pdf](https://orca.cardiff.ac.uk/id/eprint/167660/1/RAL_ICRA_2024_Qingmeng_GLSkeleton_Final%283%29.pdf)  
26. Curve Skeleton Extraction from Incomplete Point Clouds of Livestock and Its Application in Posture Evaluation \- MDPI, accessed March 27, 2025, [https://www.mdpi.com/2077-0472/12/7/998?type=check\_update\&version=1](https://www.mdpi.com/2077-0472/12/7/998?type=check_update&version=1)  
27. 3D Skeletons: A State-of-the-Art Report \- Andrea Tagliasacchi, accessed March 27, 2025, [https://theialab.ca/pubs/tagliasacchi2016star.pdf](https://theialab.ca/pubs/tagliasacchi2016star.pdf)  
28. Distance field guided L1-median skeleton extraction | Request PDF \- ResearchGate, accessed March 27, 2025, [https://www.researchgate.net/publication/309580720\_Distance\_field\_guided\_L1-median\_skeleton\_extraction](https://www.researchgate.net/publication/309580720_Distance_field_guided_L1-median_skeleton_extraction)  
29. A Practical Introduction to Skeletons for the Plant Sciences \- BioOne Complete, accessed March 27, 2025, [https://bioone.org/journals/applications-in-plant-sciences/volume-2/issue-8/apps.1400005/A-Practical-Introduction-to-Skeletons-for-the-Plant-Sciences/10.3732/apps.1400005.pdf](https://bioone.org/journals/applications-in-plant-sciences/volume-2/issue-8/apps.1400005/A-Practical-Introduction-to-Skeletons-for-the-Plant-Sciences/10.3732/apps.1400005.pdf)  
30. Actions · SmartPolarBear/l1skeleton\_py · GitHub, accessed March 27, 2025, [https://github.com/SmartPolarBear/l1skeleton\_py/actions](https://github.com/SmartPolarBear/l1skeleton_py/actions)  
31. Activity · SmartPolarBear/l1skeleton\_py · GitHub, accessed March 27, 2025, [https://github.com/SmartPolarBear/l1skeleton\_py/activity](https://github.com/SmartPolarBear/l1skeleton_py/activity)  
32. www.vovakim.com, accessed March 27, 2025, [http://www.vovakim.com/papers/24\_SIGGRAPHConference\_GEM3D.pdf](http://www.vovakim.com/papers/24_SIGGRAPHConference_GEM3D.pdf)  
33. GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis \- Dmitry Petrov, accessed March 27, 2025, [https://lodurality.github.io/GEM3D/static/GEM3D\_website.pdf](https://lodurality.github.io/GEM3D/static/GEM3D_website.pdf)  
34. P2MAT-NET: Learning Medial Axis Transform from Sparse Point ..., accessed March 27, 2025, [https://personal.utdallas.edu/\~xguo/P2MAT-NET.pdf](https://personal.utdallas.edu/~xguo/P2MAT-NET.pdf)  
35. A Comprehensive Survey on Graph Summarization With Graph Neural Networks, accessed March 27, 2025, [https://www.computer.org/csdl/journal/ai/2024/08/10382477/1TxRQYbrFTO](https://www.computer.org/csdl/journal/ai/2024/08/10382477/1TxRQYbrFTO)  
36. Neural function approximation on graphs: shape modelling, graph discrimination & compression \- CORE, accessed March 27, 2025, [https://core.ac.uk/download/pdf/573839667.pdf](https://core.ac.uk/download/pdf/573839667.pdf)  
37. Deep Encoder-Decoder Neural Network Architectures for Graph Output Signals, accessed March 27, 2025, [https://www.researchgate.net/publication/340303948\_Deep\_Encoder-Decoder\_Neural\_Network\_Architectures\_for\_Graph\_Output\_Signals](https://www.researchgate.net/publication/340303948_Deep_Encoder-Decoder_Neural_Network_Architectures_for_Graph_Output_Signals)  
38. Overparametrized Deep Encoder-Decoder Schemes for Inputs and Outputs Defined over Graphs \- ResearchGate, accessed March 27, 2025, [https://www.researchgate.net/publication/348734024\_Overparametrized\_Deep\_Encoder-Decoder\_Schemes\_for\_Inputs\_and\_Outputs\_Defined\_over\_Graphs](https://www.researchgate.net/publication/348734024_Overparametrized_Deep_Encoder-Decoder_Schemes_for_Inputs_and_Outputs_Defined_over_Graphs)  
39. A multiresolution descriptor for deformable 3D shape retrieval, accessed March 27, 2025, [https://www.researchgate.net/publication/262161314\_A\_multiresolution\_descriptor\_for\_deformable\_3D\_shape\_retrieval](https://www.researchgate.net/publication/262161314_A_multiresolution_descriptor_for_deformable_3D_shape_retrieval)  
40. graph-neural-networks.github.io, accessed March 27, 2025, [https://graph-neural-networks.github.io/static/file/chapter11.pdf](https://graph-neural-networks.github.io/static/file/chapter11.pdf)  
41. A Comprehensive Introduction to Graph Neural Networks (GNNs ..., accessed March 27, 2025, [https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial](https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial)  
42. Graph Autoencoder of arbitrary node size, how to decode? \- Data Science Stack Exchange, accessed March 27, 2025, [https://datascience.stackexchange.com/questions/130996/graph-autoencoder-of-arbitrary-node-size-how-to-decode](https://datascience.stackexchange.com/questions/130996/graph-autoencoder-of-arbitrary-node-size-how-to-decode)  
43. \[R\] Graph Autoencoder of arbitrary node size, how to decode? : r/MachineLearning \- Reddit, accessed March 27, 2025, [https://www.reddit.com/r/MachineLearning/comments/1hk7lje/r\_graph\_autoencoder\_of\_arbitrary\_node\_size\_how\_to/](https://www.reddit.com/r/MachineLearning/comments/1hk7lje/r_graph_autoencoder_of_arbitrary_node_size_how_to/)  
44. What is an encoder-decoder model? \- IBM, accessed March 27, 2025, [https://www.ibm.com/think/topics/encoder-decoder-model](https://www.ibm.com/think/topics/encoder-decoder-model)  
45. 10.6. The Encoder–Decoder Architecture — Dive into Deep Learning 1.0.3 documentation, accessed March 27, 2025, [https://d2l.ai/chapter\_recurrent-modern/encoder-decoder.html](https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html)  
46. Encoder-Decoder Long Short-Term Memory Networks \- MachineLearningMastery.com, accessed March 27, 2025, [https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/](https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/)  
47. 9.7. Sequence to Sequence Learning — Dive into Deep Learning 0.17.6 documentation, accessed March 27, 2025, [https://classic.d2l.ai/chapter\_recurrent-modern/seq2seq.html](https://classic.d2l.ai/chapter_recurrent-modern/seq2seq.html)  
48. When Dimensionality Reduction Meets Graph (Drawing) Theory: Introducing a Common Framework, Challenges and Opportunities \- arXiv, accessed March 27, 2025, [https://arxiv.org/html/2412.06555v1](https://arxiv.org/html/2412.06555v1)  
49. TEXT SUMMARIZATION USING GRAPH BASED METHODS AND DEEP LEARNING, accessed March 27, 2025, [https://www.ijprems.com/uploadedfiles/paper/issue\_12\_december\_2024/37678/final/fin\_ijprems1733848646.pdf](https://www.ijprems.com/uploadedfiles/paper/issue_12_december_2024/37678/final/fin_ijprems1733848646.pdf)  
50. Graph Summarization with Quality Guarantees \- Two Sigma, accessed March 27, 2025, [https://www.twosigma.com/wp-content/uploads/Riondato\_-\_GraphSummarization\_-\_DMKD\_-\_Paper.pdf](https://www.twosigma.com/wp-content/uploads/Riondato_-_GraphSummarization_-_DMKD_-_Paper.pdf)  
51. A Comprehensive Survey on Graph Summarization with Graph Neural Networks \- arXiv, accessed March 27, 2025, [http://arxiv.org/pdf/2302.06114](http://arxiv.org/pdf/2302.06114)  
52. \[2203.05919\] Graph Summarization with Graph Neural Networks \- arXiv, accessed March 27, 2025, [https://arxiv.org/abs/2203.05919](https://arxiv.org/abs/2203.05919)  
53. A Comprehensive Survey on Graph Summarization With Graph Neural Networks, accessed March 27, 2025, [https://www.researchgate.net/publication/377250542\_A\_Comprehensive\_Survey\_on\_Graph\_Summarization\_with\_Graph\_Neural\_Networks](https://www.researchgate.net/publication/377250542_A_Comprehensive_Survey_on_Graph_Summarization_with_Graph_Neural_Networks)