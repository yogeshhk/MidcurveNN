{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7126991,"sourceType":"datasetVersion","datasetId":4111412}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T08:18:33.513136Z","iopub.execute_input":"2023-12-05T08:18:33.513797Z","iopub.status.idle":"2023-12-05T08:18:33.875337Z","shell.execute_reply.started":"2023-12-05T08:18:33.513769Z","shell.execute_reply":"2023-12-05T08:18:33.874387Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/midcurvellm/midcurve_llm_val.csv\n/kaggle/input/midcurvellm/midcurve_llm_test.csv\n/kaggle/input/midcurvellm/midcurve_llm_train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Code T5 based EncoderDecoder training for generating Midcurve from Profile.\n\nRefer https://github.com/yogeshhk/MidcurveNN for more details, for now","metadata":{}},{"cell_type":"markdown","source":"## Installation","metadata":{}},{"cell_type":"markdown","source":"Let's first install the required libraries:\n* HuggingFace Transformers (for the CodeT5 model)\n* HuggingFace Datasets (for loading the dataset + preprocessing it)\n* PyTorch Lightning (for training)\n* Weights and Biases (for logging training metrics).","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:20:39.544604Z","iopub.execute_input":"2023-12-05T08:20:39.544992Z","iopub.status.idle":"2023-12-05T08:20:51.156930Z","shell.execute_reply.started":"2023-12-05T08:20:39.544961Z","shell.execute_reply":"2023-12-05T08:20:51.155895Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:36:26.668259Z","iopub.execute_input":"2023-12-05T08:36:26.668526Z","iopub.status.idle":"2023-12-05T08:36:42.036782Z","shell.execute_reply.started":"2023-12-05T08:36:26.668503Z","shell.execute_reply":"2023-12-05T08:36:42.035660Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nCollecting huggingface-hub>=0.18.0 (from datasets)\n  Obtaining dependency information for huggingface-hub>=0.18.0 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, huggingface-hub, datasets\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.17.3\n    Uninstalling huggingface-hub-0.17.3:\n      Successfully uninstalled huggingface-hub-0.17.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.15.0 huggingface-hub-0.19.4 pyarrow-hotfix-0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pytorch-lightning wandb","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:37:04.049447Z","iopub.execute_input":"2023-12-05T08:37:04.050402Z","iopub.status.idle":"2023-12-05T08:37:16.607581Z","shell.execute_reply.started":"2023-12-05T08:37:04.050365Z","shell.execute_reply":"2023-12-05T08:37:16.606559Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.1.1)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.0)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.24.3)\nRequirement already satisfied: torch>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (2.0.0)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.1)\nRequirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (2023.10.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.2.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.5.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.9.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.34.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lightning) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lightning) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocess data\n\nHere, we load the csv files to create a dataset.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, DatasetDict\n\nbase_url = \"/kaggle/input/midcurvellm/\"\ndataset = load_dataset(\"csv\", data_files={\"train\": base_url + \"midcurve_llm_train.csv\", \n                                          \"test\": base_url + \"midcurve_llm_test.csv\",\n                                          \"validation\": base_url + \"midcurve_llm_val.csv\"})\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:37:39.783017Z","iopub.execute_input":"2023-12-05T08:37:39.783510Z","iopub.status.idle":"2023-12-05T08:37:41.576490Z","shell.execute_reply.started":"2023-12-05T08:37:39.783455Z","shell.execute_reply":"2023-12-05T08:37:41.575664Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd86796fd426428fa31a3fba4a067269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a94b7e1ee73748b0b69860793576b1a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0cfd3e4f0448d78f2139b1f07b7073"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if _pandas_api.is_sparse(col):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc90cc37ac2438d83be28b42045449c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if _pandas_api.is_sparse(col):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6aae82d8b444d482c7fa598e2ef70a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if _pandas_api.is_sparse(col):\n","output_type":"stream"},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['ShapeName', 'Profile', 'Midcurve', 'Profile_brep', 'Midcurve_brep'],\n        num_rows: 793\n    })\n    test: Dataset({\n        features: ['ShapeName', 'Profile', 'Midcurve', 'Profile_brep', 'Midcurve_brep'],\n        num_rows: 99\n    })\n    validation: Dataset({\n        features: ['ShapeName', 'Profile', 'Midcurve', 'Profile_brep', 'Midcurve_brep'],\n        num_rows: 100\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's look at one particular example:","metadata":{}},{"cell_type":"code","source":"example = dataset['train'][0]\n\nprint(\"Profile_brep:\", example[\"Profile_brep\"])\nprint(\"Midcurve_brep:\", example[\"Midcurve_brep\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:37:51.141387Z","iopub.execute_input":"2023-12-05T08:37:51.141964Z","iopub.status.idle":"2023-12-05T08:37:51.148616Z","shell.execute_reply.started":"2023-12-05T08:37:51.141931Z","shell.execute_reply":"2023-12-05T08:37:51.147471Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Profile_brep: \"{\\\"Points\\\": [[-3.21, 6.3], [-1.67, 11.06], [-15.93, 15.69], [-17.48, 10.94]], \\\"Lines\\\": [[0, 1], [1, 2], [2, 3], [3, 0]], \\\"Segments\\\": [[0, 1, 2, 3]]}\"\nMidcurve_brep: \"{\\\"Points\\\": [[-2.44, 8.68], [-16.7, 13.31]], \\\"Lines\\\": [[0, 1]], \\\"Segments\\\": [[0]]}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The goal for the model is to generate Midcurve Brep from given Profile Brep.\n\nTo summarize:\n* input: code, which is turned into `input_ids` + `attention_mask`\n* output: docstrings, which are turned into `labels` (which are the `input_ids` of the docstrings).\n\nBelow, we define a `preprocess_examples` function, which we can apply on the entire dataset.","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n\nprefix = \"Skeletonize the Profile: \"\nmax_input_length = 256\nmax_target_length = 128\n\ndef preprocess_examples(examples):\n  # encode the code-docstring pairs\n  profiles = examples['Profile_brep']\n  midcurves = examples['Midcurve_brep']\n\n  inputs = [prefix + profile for profile in profiles]\n  model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n\n  # encode the summaries\n  labels = tokenizer(midcurves, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n\n  # important: we need to replace the index of the padding tokens by -100\n  # such that they are not taken into account by the CrossEntropyLoss\n  labels_with_ignore_index = []\n  for labels_example in labels:\n    labels_example = [label if label != 0 else -100 for label in labels_example]\n    labels_with_ignore_index.append(labels_example)\n\n  model_inputs[\"labels\"] = labels_with_ignore_index\n\n  return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:37:59.580528Z","iopub.execute_input":"2023-12-05T08:37:59.580894Z","iopub.status.idle":"2023-12-05T08:38:04.799921Z","shell.execute_reply.started":"2023-12-05T08:37:59.580866Z","shell.execute_reply":"2023-12-05T08:38:04.799082Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dceedc40ce244529b21f0d445ba02e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f012ff848c5437c9db7a7b319d6ad81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e91c6021c9949f7a98559a9ff855536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d86cfbdd2844ae90d683094ec1706e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00056b21fe54a2c84217b7138d97c27"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now that we have defined the function, let's call `.map()` on the HuggingFace Dataset object, which allows us to apply this function in batches (by default a batch size of 1,000 is used!) - hence super fast.","metadata":{}},{"cell_type":"code","source":"dataset = dataset.map(preprocess_examples, batched=True)\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:38:04.801443Z","iopub.execute_input":"2023-12-05T08:38:04.801999Z","iopub.status.idle":"2023-12-05T08:38:10.855334Z","shell.execute_reply.started":"2023-12-05T08:38:04.801969Z","shell.execute_reply":"2023-12-05T08:38:10.854400Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/793 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a8f8cc959148e2bcf0b76b8aaa45a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/99 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a9cb301cb474f9aad06b3382e868ca6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a8168e6a5a249c0b86008325e021eaa"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['ShapeName', 'Profile', 'Midcurve', 'Profile_brep', 'Midcurve_brep', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 793\n    })\n    test: Dataset({\n        features: ['ShapeName', 'Profile', 'Midcurve', 'Profile_brep', 'Midcurve_brep', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 99\n    })\n    validation: Dataset({\n        features: ['ShapeName', 'Profile', 'Midcurve', 'Profile_brep', 'Midcurve_brep', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, let's set the format to \"torch\" and create PyTorch dataloaders.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\ntrain_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=8)\nvalid_dataloader = DataLoader(dataset['validation'], batch_size=4)\ntest_dataloader = DataLoader(dataset['test'], batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:38:15.947121Z","iopub.execute_input":"2023-12-05T08:38:15.947499Z","iopub.status.idle":"2023-12-05T08:38:15.955522Z","shell.execute_reply.started":"2023-12-05T08:38:15.947459Z","shell.execute_reply":"2023-12-05T08:38:15.954642Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nprint(batch.keys())","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:38:20.601737Z","iopub.execute_input":"2023-12-05T08:38:20.602164Z","iopub.status.idle":"2023-12-05T08:38:20.668660Z","shell.execute_reply.started":"2023-12-05T08:38:20.602133Z","shell.execute_reply":"2023-12-05T08:38:20.667721Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask', 'labels'])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's verify an example, by decoding it back into text:","metadata":{}},{"cell_type":"code","source":"tokenizer.decode(batch['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:38:24.911611Z","iopub.execute_input":"2023-12-05T08:38:24.911996Z","iopub.status.idle":"2023-12-05T08:38:24.921708Z","shell.execute_reply.started":"2023-12-05T08:38:24.911955Z","shell.execute_reply":"2023-12-05T08:38:24.920564Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'<s>Skeletonize the Profile: \"{\\\\\"Points\\\\\": [[0.0, 125.0], [50.0, 125.0], [50.0, 225.0], [75.0, 225.0], [75.0, 125.0], [125.0, 125.0], [125.0, 100.0], [75.0, 100.0], [75.0, 0.0], [50.0, 0.0], [50.0, 100.0], [0.0, 100.0]], \\\\\"Lines\\\\\": [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 0]], \\\\\"Segments\\\\\": [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]}\"</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"},"metadata":{}}]},{"cell_type":"code","source":"labels = batch['labels'][0]\ntokenizer.decode([label for label in labels if label != -100])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:38:31.945558Z","iopub.execute_input":"2023-12-05T08:38:31.945929Z","iopub.status.idle":"2023-12-05T08:38:31.959801Z","shell.execute_reply.started":"2023-12-05T08:38:31.945898Z","shell.execute_reply":"2023-12-05T08:38:31.958802Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'<s>\"{\\\\\"Points\\\\\": [[62.5, 0.0], [62.5, 112.5], [62.5, 225.0], [0.0, 112.5], [125.0, 112.5]], \\\\\"Lines\\\\\": [[0, 1], [4, 1], [2, 1], [3, 1]], \\\\\"Segments\\\\\": [[0], [1], [2], [3]]}\"</s>'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine-tune using PyTorch Lightning\n\nAs we will train the model using PyTorch Lightning, we first need to define a `LightningModule`, which is an `nn.Module` with some additional functionalities. We just need to define the `forward` pass, `training_step` (and optionally `validation_step` and `test_step`), and the corresponding dataloaders. PyTorch Lightning will then automate the training for us, handling device placement (i.e. we don't need to type `.to(device)` anywhere), etc. It also comes with support for loggers (such as Tensorboard, Weights and Biases) and callbacks.\n\nOf course, you could also train the model in other ways:\n* using regular PyTorch\n* using the HuggingFace Trainer (in this case, the Seq2SeqTrainer)\n* using HuggingFace Accelerate\n* etc.","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\nimport pytorch_lightning as pl\n\nclass CodeT5(pl.LightningModule):\n    def __init__(self, lr=5e-5, num_train_epochs=15, warmup_steps=1000):\n        super().__init__()\n        self.model_name = \"Salesforce/codet5-small\" \n        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n        self.save_hyperparameters()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        return outputs\n\n    def common_step(self, batch, batch_idx):\n        outputs = self(**batch)\n        loss = outputs.loss\n\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self.common_step(batch, batch_idx)\n        # logs metrics for each training_step,\n        # and the average across the epoch\n        self.log(\"training_loss\", loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss = self.common_step(batch, batch_idx)\n        self.log(\"validation_loss\", loss, on_epoch=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        loss = self.common_step(batch, batch_idx)\n\n        return loss\n\n    def configure_optimizers(self):\n        # create optimizer\n        optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n        # create learning rate scheduler\n        num_train_optimization_steps = self.hparams.num_train_epochs * len(train_dataloader)\n        lr_scheduler = {'scheduler': get_linear_schedule_with_warmup(optimizer,\n                                                    num_warmup_steps=self.hparams.warmup_steps,\n                                                    num_training_steps=num_train_optimization_steps),\n                        'name': 'learning_rate',\n                        'interval':'step',\n                        'frequency': 1}\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n\n    def train_dataloader(self):\n        return train_dataloader\n\n    def val_dataloader(self):\n        return valid_dataloader\n\n    def test_dataloader(self):\n        return test_dataloader","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:38:39.004642Z","iopub.execute_input":"2023-12-05T08:38:39.005508Z","iopub.status.idle":"2023-12-05T08:38:40.178387Z","shell.execute_reply.started":"2023-12-05T08:38:39.005466Z","shell.execute_reply":"2023-12-05T08:38:40.177323Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Let's start up Weights and Biases!","metadata":{}},{"cell_type":"code","source":"import wandb\n\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:39:02.762343Z","iopub.execute_input":"2023-12-05T08:39:02.762978Z","iopub.status.idle":"2023-12-05T08:39:10.040823Z","shell.execute_reply.started":"2023-12-05T08:39:02.762941Z","shell.execute_reply":"2023-12-05T08:39:10.039983Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, we initialize the model.","metadata":{}},{"cell_type":"code","source":"model = CodeT5()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:39:17.867082Z","iopub.execute_input":"2023-12-05T08:39:17.868066Z","iopub.status.idle":"2023-12-05T08:39:23.929095Z","shell.execute_reply.started":"2023-12-05T08:39:17.868026Z","shell.execute_reply":"2023-12-05T08:39:23.928215Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44831571410e42279092b8f6c2ba5b1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eeb63f0a678453793ab9f37e8e578dc"}},"metadata":{}}]},{"cell_type":"markdown","source":"We can now simply start training on Colab's GPU.","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n\nwandb_logger = WandbLogger(name='codet5-finetune-midcurve', project='CodeT5')\n# for early stopping, see https://pytorch-lightning.readthedocs.io/en/1.0.0/early_stopping.html?highlight=early%20stopping\nearly_stop_callback = EarlyStopping(\n    monitor='validation_loss',\n    patience=3,\n    strict=False,\n    verbose=False,\n    mode='min'\n)\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\ntrainer = Trainer(max_epochs=5,accelerator=\"auto\",# gpus=1,\n                  default_root_dir=\"/kaggle/working/Checkpoints\",\n                  logger=wandb_logger,\n                  callbacks=[early_stop_callback, lr_monitor])\ntrainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:40:39.291667Z","iopub.execute_input":"2023-12-05T08:40:39.292054Z","iopub.status.idle":"2023-12-05T08:42:25.325291Z","shell.execute_reply.started":"2023-12-05T08:40:39.292022Z","shell.execute_reply":"2023-12-05T08:42:25.324034Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bcec244995141a482c9b2ad62b7a8c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"markdown","source":"Once we're done training, we can also save the HuggingFace model as follows:","metadata":{}},{"cell_type":"code","source":"save_directory = \"/kaggle/working/models\" # save in the current working directory, you can change this of course\nmodel.model.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:47:30.185680Z","iopub.execute_input":"2023-12-05T08:47:30.186452Z","iopub.status.idle":"2023-12-05T08:47:30.712526Z","shell.execute_reply.started":"2023-12-05T08:47:30.186420Z","shell.execute_reply":"2023-12-05T08:47:30.711160Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\nNow that we've trained a model, let's test it on some examples from the test set.","metadata":{}},{"cell_type":"code","source":"dataset_infr = load_dataset(\"csv\", data_files={\"train\": base_url + \"midcurve_llm_train.csv\", \n                                          \"test\": base_url + \"midcurve_llm_test.csv\",\n                                          \"validation\": base_url + \"midcurve_llm_val.csv\"})\nprint(dataset_infr['test'])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:47:53.963553Z","iopub.execute_input":"2023-12-05T08:47:53.964660Z","iopub.status.idle":"2023-12-05T08:47:54.120097Z","shell.execute_reply.started":"2023-12-05T08:47:53.964613Z","shell.execute_reply":"2023-12-05T08:47:54.118882Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['ShapeName', 'Profile', 'Midcurve', 'Profile_brep', 'Midcurve_brep'],\n    num_rows: 99\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"test_example = dataset_infr['test'][2]\nprint(\"Profile_brep:\", test_example['Profile_brep'])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:47:58.561279Z","iopub.execute_input":"2023-12-05T08:47:58.561727Z","iopub.status.idle":"2023-12-05T08:47:58.576146Z","shell.execute_reply.started":"2023-12-05T08:47:58.561691Z","shell.execute_reply":"2023-12-05T08:47:58.574992Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Profile_brep: \"{\\\"Points\\\": [[-6.96, 1.23], [-9.83, 5.32], [-22.12, -3.28], [-19.25, -7.38]], \\\"Lines\\\": [[0, 1], [1, 2], [2, 3], [3, 0]], \\\"Segments\\\": [[0, 1, 2, 3]]}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can load our trained model as follows:","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration\n\nmodel = T5ForConditionalGeneration.from_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:48:00.993384Z","iopub.execute_input":"2023-12-05T08:48:00.994194Z","iopub.status.idle":"2023-12-05T08:48:01.905496Z","shell.execute_reply.started":"2023-12-05T08:48:00.994155Z","shell.execute_reply":"2023-12-05T08:48:01.904372Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"We can prepare the example using `RobertaTokenizer`, and generate using the `.generate()` method. Note that there are several ways of doing generation (greedy decoding/beam search/top k sampling/etc.), for that I refer to Patrick's blog post which you can find [here](https://huggingface.co/blog/how-to-generate). Here we will just use the default settings (i.e. greedy decoding).","metadata":{}},{"cell_type":"code","source":"# prepare for the model\ninput_ids = tokenizer(test_example['Profile_brep'], return_tensors='pt').input_ids\n# generate\noutputs = model.generate(input_ids)\nprint(\"Generated Midcurve:\", tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:48:10.814648Z","iopub.execute_input":"2023-12-05T08:48:10.815019Z","iopub.status.idle":"2023-12-05T08:48:11.266530Z","shell.execute_reply.started":"2023-12-05T08:48:10.814990Z","shell.execute_reply":"2023-12-05T08:48:11.265479Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated Midcurve: \"{\\\"Points\\\": [[-8.83, 4.87], [-21.23\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's compare this to the ground-truth docstring:","metadata":{}},{"cell_type":"code","source":"print(\"Ground truth:\", test_example['Midcurve_brep'])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:48:20.016209Z","iopub.execute_input":"2023-12-05T08:48:20.017110Z","iopub.status.idle":"2023-12-05T08:48:20.024157Z","shell.execute_reply.started":"2023-12-05T08:48:20.017075Z","shell.execute_reply":"2023-12-05T08:48:20.022896Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Ground truth: \"{\\\"Points\\\": [[-8.4, 3.28], [-20.68, -5.33]], \\\"Lines\\\": [[0, 1]], \\\"Segments\\\": [[0]]}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}